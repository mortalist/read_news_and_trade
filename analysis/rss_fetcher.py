"""
RSS ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ
ì—¬ëŸ¬ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì—ëŸ¬ ì²˜ë¦¬ ì œê³µ
"""
import feedparser
import time
from typing import List, Dict, Set
from datetime import datetime


class RSSFetcher:
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° ë° ìºì‹± í¬í•¨)"""

    def __init__(self, feed_urls: List[str], limit_per_feed: int = 5, cache_expiration_seconds: int = 3600):
        """
        Args:
            feed_urls: RSS í”¼ë“œ URL ë¦¬ìŠ¤íŠ¸
            limit_per_feed: ê° í”¼ë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜
            cache_expiration_seconds: ìºì‹œ ìœ ì§€ ì‹œê°„ (ì´ˆ, ê¸°ë³¸ 1ì‹œê°„)
        """
        self.feed_urls = feed_urls
        self.limit_per_feed = limit_per_feed
        self.cache_expiration_seconds = cache_expiration_seconds
        self.cached_article_url_timestamps: Dict[str, float] = {}

    def fetch_all_news(self) -> List[Dict]:
        """
        ëª¨ë“  RSS í”¼ë“œì—ì„œ ìƒˆ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° ë° ìºì‹±)

        Returns:
            ìƒˆ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ [{'title', 'published', 'summary', 'link', 'source'}, ...]
        """
        self._clean_cache()

        all_articles = []
        deduplicated_title_set: Set[str] = set()

        for feed_url in self.feed_urls:
            try:
                # RSS í”¼ë“œ íŒŒì‹±
                feed = feedparser.parse(feed_url)

                # íŒŒì‹± ì˜¤ë¥˜ ì²´í¬
                if feed.bozo:
                    print(f"âš ï¸ RSS íŒŒì‹± ê²½ê³  [{feed_url}]: {feed.bozo_exception}")

                # í”¼ë“œ ì†ŒìŠ¤ ì´ë¦„ ì¶”ì¶œ (í”¼ë“œ ì œëª© ë˜ëŠ” URL)
                source = feed.feed.get('title', feed_url)

                # ì œí•œëœ ìˆ˜ë§Œí¼ ê¸°ì‚¬ ìˆ˜ì§‘
                entries = feed.entries[:self.limit_per_feed]
                newly_collected_count = 0

                for entry in entries:
                    link = entry.get('link', '')
                    title = entry.get('title', 'No title')

                    if link in self.cached_article_url_timestamps:
                        continue

                    normalized_title_key = title.lower().strip()[:100]
                    if normalized_title_key in deduplicated_title_set:
                        continue

                    deduplicated_title_set.add(normalized_title_key)

                    article = {
                        'title': title,
                        'published': entry.get('published', 'Unknown date'),
                        'summary': entry.get('summary', entry.get('description', 'No summary')),
                        'link': link,
                        'source': source
                    }
                    all_articles.append(article)

                    self.cached_article_url_timestamps[link] = time.time()
                    newly_collected_count += 1

                if newly_collected_count > 0:
                    print(f"âœ… [{source}] {newly_collected_count}ê°œ ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘")
                else:
                    print(f"â„¹ï¸ [{source}] ìƒˆ ê¸°ì‚¬ ì—†ìŒ (ìºì‹œì— ì´ë¯¸ ì¡´ì¬)")

                # Rate limiting ë°©ì§€
                time.sleep(0.5)

            except Exception as e:
                print(f"âŒ RSS ìˆ˜ì§‘ ì‹¤íŒ¨ [{feed_url}]: {e}")
                continue

        print(f"\nğŸ“Š ì´ {len(all_articles)}ê°œ ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° ë° ìºì‹± ì™„ë£Œ)")
        return all_articles

    def _clean_cache(self):
        """ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì‚­ì œ"""
        current_timestamp = time.time()
        expired_article_urls = [
            article_url for article_url, cached_timestamp in self.cached_article_url_timestamps.items()
            if current_timestamp - cached_timestamp > self.cache_expiration_seconds
        ]

        for article_url in expired_article_urls:
            del self.cached_article_url_timestamps[article_url]

        if expired_article_urls:
            print(f"ğŸ§¹ ìºì‹œ ì •ë¦¬: {len(expired_article_urls)}ê°œ í•­ëª© ì‚­ì œ")
